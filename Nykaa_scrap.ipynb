{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-2417888062d4>:8: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Empty DataFrame\n",
      "Columns: [Product_Id, Category_Id, Name, Featured, Url, Discount, Marked Price, Quantity, Rating(out of 5), Review Count, Rating Count, Category, SubCategory1, SubCategory2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(chrome_options=options)\n",
    "\n",
    "search=pd.read_excel('output.xlsx',sheet_name='Sheet2')\n",
    "search=search['urls'].head(25)\n",
    "product=[]\n",
    "product_list=[]\n",
    "product_details=[]\n",
    "quantity=[]\n",
    "rating=[]\n",
    "Category=[]\n",
    "Sub_Category=[]\n",
    "Sub_Category2=[]\n",
    "total_reviews=[]\n",
    "total_ratings=[]\n",
    "price=[]\n",
    "\n",
    "def printurl(url,j):\n",
    "    if j<5: #No of pages to be scrapped for each catgeory\n",
    "        driver.get(url)\n",
    "        page = requests.get(url)\n",
    "        soup=BeautifulSoup(page.content, 'html.parser')\n",
    "        name=[]\n",
    "        for i in soup.find_all('h2'):\n",
    "            name.append(i['title'])\n",
    "        Featured=[]\n",
    "        for i in soup.find_all('div',class_= 'tags-offer-container'):\n",
    "            if i.find('span',class_='featured'):\n",
    "                Featured.append(\"Yes\")\n",
    "            else:\n",
    "                Featured.append(\"No\")\n",
    "        links=[]\n",
    "        productId=[]\n",
    "        categoryId=[]\n",
    "        for l in soup.find_all('div',class_='product-list-box'):\n",
    "            link=l.find('a')['href']\n",
    "            if(l.find('h2') is not None):\n",
    "                k=\"https://www.nykaa.com\"+link\n",
    "                links.append(k)\n",
    "                try:\n",
    "                    c=k.split(\"skuId\")[1].replace(\"=\",\"\").split(\"&\")[0]\n",
    "                    productId.append(c)  \n",
    "                except:\n",
    "                    productId.append('-') \n",
    "                try:\n",
    "                    c=k.split(\"categoryId\")[1].replace(\"=\",\"\").split(\"&\")[0]\n",
    "                    categoryId.append(c)  \n",
    "                except:\n",
    "                    categoryId.append('-') \n",
    "\n",
    "                    \n",
    "        discount=[]    \n",
    "        for s in soup.find_all('div',class_='m-content__product-list__price'):\n",
    "            if s.find('div',class_='discount-info'):\n",
    "                discount.append(s.find('div',class_='discount-info').find('span').text)\n",
    "            else:\n",
    "                discount.append(\"-\")\n",
    "        product.append(list(zip(productId,categoryId,name,Featured,links,discount)))\n",
    "        try:\n",
    "            lis=driver.find_element_by_css_selector('li.next a')\n",
    "            next_url=lis.get_attribute('href')\n",
    "            printurl(next_url,j+1)\n",
    "            print(j)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "for u in search:\n",
    "    printurl(u,0)\n",
    "    for i in product:\n",
    "        for j in i:\n",
    "            product_list.append(j)\n",
    "            product=[]\n",
    "print(len(product_list))\n",
    "\n",
    "#Second page scrapping\n",
    "def getdetails2(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        page = requests.get(url)\n",
    "        soup=BeautifulSoup(page.content, 'html.parser')\n",
    "        try:\n",
    "            quantity.append(driver.find_element_by_css_selector(\"span.shade-txt\").text.replace('(','').replace(')',''))    \n",
    "        except:\n",
    "            quantity.append(\"-\")\n",
    "        try:\n",
    "            price.append(int(soup.find(\"span\",class_=\"post-card__content-price-offer\").text.replace(\"â‚¹\",\"\")))\n",
    "        except:\n",
    "            price.append(\"-\")\n",
    "        try:\n",
    "            rating.append(soup.find(\"div\",class_=\"m-content__product-list__ratings\").find(\"meta\")['content'])\n",
    "        except:\n",
    "            rating.append(\"-\")\n",
    "        try:\n",
    "            total=int(soup.find(\"div\",class_=\"m-content__product-list__ratings\").find_all(\"meta\")[2]['content'])\n",
    "        except:\n",
    "            total=0\n",
    "        total_reviews.append(total)\n",
    "        try:\n",
    "            total=int(soup.find(\"div\",class_=\"rating-count-popup\").find(\"meta\")['content'])\n",
    "        except:\n",
    "            total=0\n",
    "        total_ratings.append(total)\n",
    "        try:\n",
    "            lis=driver.find_elements_by_tag_name(\"ol li\")\n",
    "            cat=[]\n",
    "            for i in lis:\n",
    "                k=i.find_elements_by_tag_name(\"a\")\n",
    "                for j in k:\n",
    "                    cat.append(j.get_attribute('innerHTML'))\n",
    "            if len(cat)>=2:\n",
    "                Category.append(cat[1])\n",
    "            else:\n",
    "                Category.append(\"-\")\n",
    "            if len(cat)>=3:\n",
    "                Sub_Category.append(cat[2])\n",
    "            else:\n",
    "                Sub_Category.append(\"-\")\n",
    "            if len(cat)>=4:\n",
    "                Sub_Category2.append(cat[3])\n",
    "            else:\n",
    "                Sub_Category2.append(\"-\")\n",
    "        except:\n",
    "            Category.append(\"-\")\n",
    "            Sub_Category.append(\"-\")\n",
    "            Sub_Category2.append(\"-\")\n",
    "    except:\n",
    "        quantity.append(\"-\")\n",
    "        price.append(\"-\")\n",
    "        rating.append(\"-\")\n",
    "        total_reviews.append(0)\n",
    "        total_ratings.append(0)\n",
    "        Category.append(\"-\")\n",
    "        Sub_Category.append(\"-\")\n",
    "        Sub_Category2.append(\"-\")\n",
    "        \n",
    "quantity=[]\n",
    "rating=[]\n",
    "Category=[]\n",
    "Sub_Category=[]\n",
    "Sub_Category2=[]\n",
    "total_reviews=[]\n",
    "total_ratings=[]\n",
    "price=[]\n",
    "t=0\n",
    "for k in product_list:\n",
    "    getdetails2(k[3])\n",
    "    print(t)\n",
    "    t=t+1\n",
    "\n",
    "product_details.append(list(zip(price,quantity,rating,total_reviews,total_ratings,Category,Sub_Category,Sub_Category2)))\n",
    "product_specs=[]\n",
    "for i in product_details:\n",
    "    for j in i:\n",
    "        product_specs.append(j)\n",
    "product_specs=[]\n",
    "for i in product_details:\n",
    "    for j in i:\n",
    "        product_specs.append(j)\n",
    "\n",
    "results=[]\n",
    "for a,b in zip(product_list,product_specs):\n",
    "    results.append(a+b)\n",
    "df1= pd.DataFrame(results, columns =['Product_Id','Category_Id','Name', 'Featured','Url','Discount', 'Marked Price','Quantity','Rating(out of 5)','Review Count','Rating Count','Category','SubCategory1','SubCategory2'])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=search['urls'].tail(25)\n",
    "product=[]\n",
    "product_list=[]\n",
    "product_details=[]\n",
    "quantity=[]\n",
    "rating=[]\n",
    "Category=[]\n",
    "Sub_Category=[]\n",
    "Sub_Category2=[]\n",
    "total_reviews=[]\n",
    "total_ratings=[]\n",
    "price=[]\n",
    "\n",
    "for u in search:\n",
    "    printurl(u,0)\n",
    "    for i in product:\n",
    "        for j in i:\n",
    "            product_list.append(j)\n",
    "            product=[]\n",
    "print(len(product_list))\n",
    "\n",
    "quantity=[]\n",
    "rating=[]\n",
    "Category=[]\n",
    "Sub_Category=[]\n",
    "Sub_Category2=[]\n",
    "total_reviews=[]\n",
    "total_ratings=[]\n",
    "price=[]\n",
    "t=0\n",
    "for k in product_list:\n",
    "    getdetails2(k[3])\n",
    "    print(t)\n",
    "    t=t+1\n",
    "\n",
    "product_details.append(list(zip(price,quantity,rating,total_reviews,total_ratings,Category,Sub_Category,Sub_Category2)))\n",
    "product_specs=[]\n",
    "for i in product_details:\n",
    "    for j in i:\n",
    "        product_specs.append(j)\n",
    "product_specs=[]\n",
    "for i in product_details:\n",
    "    for j in i:\n",
    "        product_specs.append(j)\n",
    "\n",
    "results=[]\n",
    "for a,b in zip(product_list,product_specs):\n",
    "    results.append(a+b)\n",
    "df2= pd.DataFrame(results, columns =['Product_Id','Name', 'Featured','Url','Discount', 'Marked Price','Quantity','Rating(out of 5)','Review Count','Rating Count','Category','SubCategory1','SubCategory2'])\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2]\n",
    "df = pd.concat(frames)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brand_n_Ingredient_categorisation\n",
    "df[\"Name\"]=df[\"Name\"].str.lower()\n",
    "df[\"Name\"]=df[\"Name\"].str.replace(\".\",\"\")\n",
    "brand=pd.read_excel('brands.xlsx')\n",
    "brand_list=brand['Brand'].to_list()\n",
    "for i in brand_list:\n",
    "    i.replace(\".\",\"\")\n",
    "def matcher(x):\n",
    "    for i in ing_list:\n",
    "        if i.lower() in x.lower():\n",
    "            return i\n",
    "    else:\n",
    "        return np.nan\n",
    "word_list = brand_list\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in word_list)\n",
    "df1 = df['Name'].str.lower().str.extractall('(' + pat + ')', flags = re.I)[0].unstack().add_prefix('Brand_')\n",
    "df_col = pd.concat([df,df1], axis=1)\n",
    "ingredient=pd.read_excel('ingredient_list.xlsx')\n",
    "ing_list=ingredient['Ingredients'].to_list()\n",
    "word_list2 = ing_list\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in word_list2)\n",
    "df2 = df['Name'].str.lower().str.extractall('(' + pat + ')', flags = re.I)[0].unstack().add_prefix('Ingredient_')\n",
    "df_col_2 = pd.concat([df_col,df2], axis=1)\n",
    "\n",
    "from datetime import date\n",
    "today =  str(date.today())\n",
    "df_col_2.to_excel(\"Nykaa \"+today+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
